# ğŸš€ Spark Ã— LocalStack â€” End-to-End Data Pipeline

[![Python 3.11](https://img.shields.io/badge/python-3.11-blue.svg)](https://www.python.org/)
[![PySpark 3.5](https://img.shields.io/badge/pyspark-3.5.1-orange.svg)](https://spark.apache.org/)
[![LocalStack](https://img.shields.io/badge/localstack-3.4-green.svg)](https://localstack.cloud/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

> A production-grade, **fully runnable** data engineering project demonstrating a **Medallion Architecture** (Bronze â†’ Silver â†’ Gold) with **PySpark** and **AWS S3** (emulated via **LocalStack**), all orchestrated through **Docker Compose** and validated by a **GitHub Actions CI/CD** pipeline.

---

## ğŸ“ Architecture

## Architecture

[![Architecture Diagram](architecture.png)](architecture.png)

Diagrama interativo (HTML): [Abrir](docs/architecture.html)

### Medallion Layers

| Layer      | Format  | Contents                                      | Partitioning              |
|------------|---------|-----------------------------------------------|---------------------------|
| **Bronze** | Parquet | Raw validated CSV + ingestion metadata        | `order_year / order_month` |
| **Silver** | Parquet | Deduplicated, cleaned, enriched with RFM cols | `year / month / category`  |
| **Gold**   | Parquet | 3 aggregated data marts (analytics-ready)     | none (small datasets)      |

### Gold Data Marts

| Mart                   | Description                                    |
|------------------------|------------------------------------------------|
| `sales_by_region`      | Monthly revenue KPIs per region with rankings  |
| `product_performance`  | Top products per category with RFM context     |
| `customer_segments`    | RFM-based customer segmentation (5 tiers)      |

---

## âš¡ Quick Start (One Command Demo)

```bash
# 1. Clone
git clone https://github.com/YOUR_USERNAME/spark-localstack-pipeline.git
cd spark-localstack-pipeline

# 2. Copy env file (defaults work out-of-the-box)
cp .env.example .env

# 3. Full demo: start stack + generate data + run pipeline
make demo
```

That's it! Open **http://localhost:8080** to see the Spark UI.

---

## ğŸ”§ Prerequisites

| Tool         | Version  | Install                            |
|--------------|----------|------------------------------------|
| Docker       | â‰¥ 24     | https://docs.docker.com/get-docker |
| Docker Compose | â‰¥ 2.20 | included with Docker Desktop       |
| Python       | â‰¥ 3.11   | https://python.org                 |
| GNU Make     | any      | `brew install make` / apt          |
| AWS CLI      | â‰¥ 2      | (optional) for inspecting S3       |

---

## ğŸ“‹ All Make Commands

```bash
make help           # Show all commands

# Infrastructure
make up             # Start LocalStack + Spark cluster
make down           # Stop everything
make build          # Rebuild Docker images from scratch
make logs           # Follow all container logs
make shell          # Open bash inside pipeline-runner
make status         # Show containers + S3 buckets

# Data Generation
make data           # Generate 50k synthetic orders â†’ S3 bronze
make data-small     # Generate 5k orders (faster for testing)

# Pipeline
make pipeline           # Full pipeline: ingestion + transform + load
make pipeline-ingestion # Step 1 only: CSV â†’ Bronze Parquet
make pipeline-transform # Step 2 only: Bronze â†’ Silver Parquet
make pipeline-load      # Step 3 only: Silver â†’ Gold marts

# Testing
make install-dev    # Install dev dependencies locally
make test           # Run unit tests
make test-cov       # Run tests with HTML coverage report

# Code Quality
make lint           # Ruff linter
make format         # Black + Ruff auto-format

# Cleanup
make clean          # Remove caches, generated files
make demo           # ğŸ¬ One-shot: up + data-small + pipeline
```

---

## ğŸ§ª Testing

Tests run **without Docker** using a local SparkSession and `moto` to mock S3:

```bash
# Install dependencies
make install-dev

# Run tests
make test

# With coverage
make test-cov
# â†’ open htmlcov/index.html
```

### Test structure

```
tests/
â”œâ”€â”€ conftest.py           # SparkSession fixture + sample DataFrame
â”œâ”€â”€ test_ingestion.py     # Schema validation, metadata enrichment
â”œâ”€â”€ test_transformation.py # Deduplication, cleaning, Silver columns
â””â”€â”€ test_load.py          # Gold mart aggregations, RFM segmentation
```

---

## ğŸ—ï¸ Project Structure

```
spark-localstack-pipeline/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml              # GitHub Actions: lint â†’ test â†’ docker â†’ e2e
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ spark/
â”‚       â”œâ”€â”€ Dockerfile          # Bitnami Spark + Hadoop AWS + PySpark deps
â”‚       â””â”€â”€ spark-defaults.conf # S3A / LocalStack tuning
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ jobs/
â”‚   â”‚   â”œâ”€â”€ ingestion.py        # Bronze layer: CSV â†’ Parquet
â”‚   â”‚   â”œâ”€â”€ transformation.py   # Silver layer: clean + dedupe + enrich
â”‚   â”‚   â””â”€â”€ load.py             # Gold layer: 3 aggregated marts
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ spark_session.py    # SparkSession factory (S3A pre-configured)
â”‚   â”‚   â””â”€â”€ aws_helpers.py      # S3 client, bucket helpers
â”‚   â””â”€â”€ pipeline.py             # Orchestrator (CLI entry point)
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ conftest.py
â”‚   â”œâ”€â”€ test_ingestion.py
â”‚   â”œâ”€â”€ test_transformation.py
â”‚   â””â”€â”€ test_load.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ init_localstack.sh      # Auto-creates S3 buckets on stack start
â”‚   â””â”€â”€ generate_data.py        # Faker-based synthetic data generator
â”œâ”€â”€ data/sample/                # Local CSV landing zone (gitignored)
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Makefile
â”œâ”€â”€ pyproject.toml              # Ruff + Black + Pytest config
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ requirements-dev.txt
â””â”€â”€ .env.example
```

---

## ğŸ”„ CI/CD Pipeline

```
Push / PR
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Lint   â”‚â”€â”€â”€â–ºâ”‚ Unit Tests â”‚â”€â”€â”€â–ºâ”‚ Docker Build â”‚â”€â”€â”€â–ºâ”‚   E2E    â”‚
â”‚ (ruff + â”‚    â”‚  (pytest + â”‚    â”‚ (Spark image)â”‚    â”‚(LocalStackâ”‚
â”‚  black) â”‚    â”‚   moto)    â”‚    â”‚              â”‚    â”‚  + Spark) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                            â”‚
                                               (on main branch only)
                                                            â”‚
                                                     â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                                                     â”‚   Publish   â”‚
                                                     â”‚  (on tag v*)â”‚
                                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- **Unit tests** run on every push/PR (no Docker required â€” uses moto)
- **E2E tests** run on merges to `main` (full LocalStack + Spark stack)
- **Docker publish** triggers on `git tag v*`

---

## ğŸ“Š Inspecting Results with AWS CLI

```bash
# List S3 contents
aws --endpoint-url=http://localhost:4566 s3 ls s3://pipeline-data/ --recursive

# Count Parquet files per layer
aws --endpoint-url=http://localhost:4566 s3 ls s3://pipeline-data/gold/ --recursive | wc -l

# Download a Gold mart locally
aws --endpoint-url=http://localhost:4566 s3 cp \
  s3://pipeline-data/gold/sales_by_region/ ./local_gold/ --recursive
```

---

## ğŸ¤ Contributing

1. Fork the repo
2. Create a feature branch: `git checkout -b feat/my-feature`
3. Make your changes + add tests
4. Run `make lint test` before committing
5. Open a Pull Request

---

## ğŸ“„ License

MIT â€” see [LICENSE](LICENSE).
