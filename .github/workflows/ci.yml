name: CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      run_e2e:
        description: 'Run end-to-end tests with LocalStack'
        required: false
        default: 'false'
        type: choice
        options: ['true', 'false']

env:
  PYTHON_VERSION: "3.11"
  JAVA_VERSION:   "17"

jobs:
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  #  1.  Code Quality
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  lint:
    name: ðŸ” Lint & Format
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: lint-${{ hashFiles('requirements-dev.txt') }}

      - name: Install linting tools
        run: pip install ruff black

      - name: Ruff lint
        run: ruff check src/ tests/ scripts/

      - name: Black format check
        run: black --check src/ tests/ scripts/

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  #  2.  Unit Tests (no Docker needed)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  unit-tests:
    name: ðŸ§ª Unit Tests
    runs-on: ubuntu-latest
    needs: lint

    steps:
      - uses: actions/checkout@v4

      - name: Set up Java ${{ env.JAVA_VERSION }}
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: temurin

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: unit-${{ hashFiles('requirements.txt', 'requirements-dev.txt') }}

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-dev.txt

      - name: Run unit tests with coverage
        env:
          AWS_ACCESS_KEY_ID:     test
          AWS_SECRET_ACCESS_KEY: test
          AWS_DEFAULT_REGION:    us-east-1
          SPARK_LOCAL_IP:        127.0.0.1
          PYSPARK_PYTHON:        python3
        run: |
          pytest tests/ \
            -v \
            --cov=src \
            --cov-report=xml \
            --cov-report=term-missing \
            --tb=short \
            -p no:warnings

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          file: coverage.xml
          fail_ci_if_error: false

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  #  3.  Docker Build
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  docker-build:
    name: ðŸ³ Docker Build
    runs-on: ubuntu-latest
    needs: unit-tests

    steps:
      - uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build Spark image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: docker/spark/Dockerfile
          push: false
          tags: spark-pipeline:ci
          cache-from: type=gha
          cache-to:   type=gha,mode=max

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  #  4.  End-to-End Test (LocalStack + Spark)
  #      Runs on push to main OR when manually triggered
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  e2e-tests:
    name: ðŸš€ End-to-End Tests
    runs-on: ubuntu-latest
    needs: docker-build
    if: >
      github.ref == 'refs/heads/main' ||
      github.event.inputs.run_e2e == 'true'

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Python dependencies (local runner)
        run: pip install -r requirements.txt

      - name: Start Docker Compose stack
        run: |
          docker compose up -d localstack spark-master spark-worker-1 pipeline-runner
          echo "Waiting for LocalStack..."
          timeout 120 bash -c 'until curl -sf http://localhost:4566/_localstack/health; do sleep 3; done'
          echo "LocalStack ready âœ…"

      - name: Generate synthetic data
        run: |
          python scripts/generate_data.py \
            --rows 5000 \
            --output data/sample/sales_raw.csv \
            --endpoint http://localhost:4566

      - name: Run full pipeline inside container
        run: |
          docker compose exec -T pipeline-runner bash -c "
            cd /opt/spark/work-dir && \
            SPARK_MASTER_URL=local[*] \
            AWS_ENDPOINT_URL=http://localstack:4566 \
            python -m src.pipeline --steps ingestion,transformation,load
          "

      - name: Verify Gold layer outputs
        run: |
          python - <<'EOF'
          import boto3, sys
          s3 = boto3.client("s3", endpoint_url="http://localhost:4566",
                            aws_access_key_id="test", aws_secret_access_key="test")
          for mart in ["sales_by_region", "product_performance", "customer_segments"]:
              prefix = f"gold/{mart}/"
              objs = s3.list_objects_v2(Bucket="pipeline-data", Prefix=prefix)
              count = objs.get("KeyCount", 0)
              if count == 0:
                  print(f"âŒ No files in gold/{mart}/")
                  sys.exit(1)
              print(f"âœ… gold/{mart}/: {count} files")
          EOF

      - name: Print container logs on failure
        if: failure()
        run: docker compose logs --tail=100

      - name: Tear down
        if: always()
        run: docker compose down -v

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  #  5.  Publish (optional â€” push to Docker Hub)
  #      Only on tagged releases
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  publish:
    name: ðŸ“¦ Publish Image
    runs-on: ubuntu-latest
    needs: e2e-tests
    if: startsWith(github.ref, 'refs/tags/v')

    steps:
      - uses: actions/checkout@v4

      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: .
          file: docker/spark/Dockerfile
          push: true
          tags: |
            ${{ secrets.DOCKERHUB_USERNAME }}/spark-localstack-pipeline:${{ github.ref_name }}
            ${{ secrets.DOCKERHUB_USERNAME }}/spark-localstack-pipeline:latest
